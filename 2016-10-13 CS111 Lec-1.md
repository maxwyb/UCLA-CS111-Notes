### COM SCI 111, Lec 10/13/2016, Part 1
### Paging & Virtual Memory
**Demand paging**: Move pages onto and off of disk on demand, as a process doesn't need all its pages in memory to run.
Start a process with a subset of its pages
MMU support *Not Present* pages: generates a fault/trap when they are referenced.
	While a process is blocked waiting for its page, it is temporarily removed from ready queue and another process will be running.

Acceptable performance: based on *locality of reference*
	The next address you ask for is likely to be clase to the last address you asked for
	1. Instruction: executed usually in sequential order
	2. Stack: usually need access to only the current stack frame (Program counter, etc.)
	3. Heap: usually reference to recently allocated structures
	Key to performance is choosing which page to "kick out" to disk, saving time when retrieving other pages from disk later
	
**Page fault**: a page is currently on disk, not in RAM.
Page fault handling:
`CPU fault --> fault enters kernel --> forwarded to page fault handler --> determine where the page is resided --> schedule I/O for fetching, the block the process --> update page table for the new-fetched page --> retry failed instruction in user-mode`

**Virtual memory**: generalization of what demand paging allows
A very large quantity of memory, for each process; all directly accessible via normal addressing, at a speed approaching that of actual RAM.
Allow each process to *request* segment within each virtual memory space.

### Managing **Swap space**
*Page replacement* policies: Eg. when a process exits, when we have a page fault, etc.
An *Optimal Replacement Algorithm*: replace the page that will be next referenced furthest in the future
	Impossible to have one, but not absolutely needed.
- *Random, FIFO*
- *Least Frequently Used (LFU)*
- *Least Recently Used (LRU)**: the general algorithm

LFU algorithm example: a 2-dimensional array-like data structure, keeping track of page loaded in each frame and last time it's used.

Maintaining Information for LRU: need a cheap software substitute, without giving extra page faults when loading each page on the first time
**Clock Algorithm**: for each page, asks MMU if page has been loaded into memory. If it has, skip this physical memory page frame next time. If not, consider the page on the current memory page to be least recently used. Replace that.  

Effective range of Page replacements & Multiprocesses:
*Single Global Page Frame Pool*: treat the entire set of page frames as a shared resource
	Bad interaction with round-robin scheduling: pages of the process last in the scheduling queue will all be swapped out	
*Pre-Process Page Frame Pools*: difficult to choose how many page frames per process.

**Working Sets**: give each running process an allocation of page frames
`Assign page frames to each in-memory process --> observe faults per unit time --> dynamically adjusting number of assigned page frames`
*Page stealing algorithm*: processes that need more pages tend to get more

**Thrashing**: working set exceeds available memory
A solution: reduce number of competing processes by swapping some ready processes out
When a swapped process comes in to memory from disk: preload the last working set
	Fewer pages to be read in than pure swapping
	Far fewer iniital page faults than pure demand paging
	
*Clean & Dirty Pages*: if the page in memory has been modified compared to the copy on disk
Great to just replace clean pages (no expensive disk written process), but inflexible
*Pre-emptive page laundering*: converting dirty pages to clean ones by writting to disk in background; an outgoing equivalent of pre-loading


