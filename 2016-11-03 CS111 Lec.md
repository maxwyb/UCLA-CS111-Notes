## COM SCI 111, Lec 11/03/2016
### Methods for Synchronization
**Monitors (Protected classes)**: each monitor *class* has a semaphore; automatically acquired on method invocation and released on method return
	Being conservative by eliminating parallelism
	+ Correctness: complete mutual exculsion
	+ Fairness: semaphore queue prevents starvation
	- Progress: inter-class dependencies may cause deadlocks
	- Performance: coarse-grained locking
	
**Java synchronized methods**: each *object* has an associated mutex, acquired before calling a *synchronized method*.
	No thread an enter the object's resources when the object is locked
	static synchronized methods lock class mutex.
	- Correctness: depend on developers
	- Fariness: Priority thread scheduling causes potential starvation
	+ Progress: safe from single thread deadlocks
	+ Performance: finer-grained locking
	
### Performance Measurement and Analysis: General
Quantify, understand and predict system performance
	compare to other systems; investigate alternatives
	determine how your system will scale up
	
Why performance analysis is hard? Components operate in a complex system:
	Many components in every process
	Ongoing competition for all resources
	System may be too large to replicate in lab, or have non-reproduceable properties
	Difficulty of making clear/simple assertions

*Design for performance*
	Anticipate bottlenecks:
		frequent operations (interrupts, copies, updates, etc.)
		limiting resources (network/disk bandwith, etc.)
		traffic concentration points (resource locks)
	design to minimize problems: reduce use or add resources
	include performance measurements when designing a system:
		what will be measured, and how
		
### Performance Measurement: Terminology
**Metrics** a measurable and quantifiable quantity; describe an important phenomenon in a system, relevant to the questions we are addressing.
	Duration/response time: how long did the process run?
	Processing rate: Eg. handling web requests
	Resource consumption
	Reliability: how many messages were delivered without error?

Choosing a set of metrics: completeness, non-redundancy, variability (show meaningful performance variation of system), feasibility (to accurately measure)

*Variability in metrics*: comes from
	Inconsisteny test conditions: varying platforms, *start-up & cache effects*
	Flawed measurement techniques: indirect measurement led to aggregate effects
	Non-deterministic factors: queuing of processes, network, etc.
*Tendency*: common or characteristic of all readings
	Indices of Tendency: mean, median, mode, etc.
*Dispersion*: variation of various measurements
	Indices of Dispersion: range, standard deviation, coefficient of variance, confidence level, etc.
	
Meaningful measurements:
	1. Measure under controlled conditions: on a specified platform, under a calibrated load, removing extraneous external influences
	2. Do direct measurements of key characteristics
	3. Ensure quality of results: cross-comparability, repeatibility
	
**Factors**: controlled variations for comparative purposes
	*level* describes values you test for each factor: numerical or categorical 
	Efforts = number-of-factors * number-of-levels
	
**Workload**: work applied to the testing system, preferably similar to the work we care about
	Simulated workload, real workload, standard benchmarks, replayed trace
	
*Simulated workload*: artificial load generation
	+ controllable operations, scalable to produce arbitrarily large or small loads
	- may not create all realistic situations; Eg. random traffic is not a usuage scenario 
*Replayed workload (replayed trace)*: captured operations from real systems
	+ represent real usage scenarios; can be replayed
	- hard to obtain
	- not necessarily scalable, limited ability to exercise little-used features
*Live workload*: instrumented systems serving clients
	+ real combinations of real scenarios
	- limited testing opportunities due to customers' demands for good performance and reliability
	- load cannot be repeated or scaled on demand
*Standard benchmarks*: carefully crafted/reviewed simulators
	+ believed to be representative of real usage
	+ standardized and widely available, well maintained regarding bugs
	+ allows comparison of competing products
	- may not be updated to simulate workloads encountered nowadays
	
### Performance Measurement: Dealing with Performance Problems
Types of performance problems:
	Non-scalable problems: Eg. cos per operation increases tremendously at scale
	Bottlenecks
	Accumulated costs
	
Common measurement mistakes:
	1. measuring time but not utilization: Eg. heavily/lightly loaded system
	2. capturing averages rather than distributions: Eg. outliers
	3. Ignoring start-up, accumulation and cache effects
	4. Ignoring instrumentation artifacts

*Measurement artifacts*: costs of instrumentation code and of logging results
	minimize frequency and costs of measuring
