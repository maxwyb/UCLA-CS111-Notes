## COM SCI 111, Lec 12/01/2016
### Remote Data: Introduction
Goals: transparency (indistinguishable from local files), performance (scalability), cost
Remote File System - remote storage devices, remote databases

Client/Server Models:
Peer-to-peer; *Thin client* (which usually has not persistent storage), cloud services

**Remote File Transfer**
Explicit commands to copy remote files: `scp`, `SFTP`
implicit remote data transfers: browsers (via HTTP), emails (via IMAP/POP/SMTP)
+ efficient, requires no OS support
- latency, lacks transparency

**Remote Disk Access**: normal fie system calls work on remote files
Typical architecutures: SCSI over Fibre Chanel (fast), iSCI (inexpensive, highly scalable) 
The model for *virtual machines*
+ complete transparency
+ reliability/availability per back-end
- inefficient fixed partition space allocation for each client; can't support file sharing between client systems
- message loss causes file system errors

**Remote File Access**
Plug-in file system architecture; client-side file system is a local proxy
THe model for *client/server storage*
+ good application level transparency
+ add support for multi-client file sharing
- at least part of implementation must be in the OS; client and server sides tend to be complex

**Cloud Model**: logical extension of cient/server model
Server is targeted to provide service to large number of clients, requiring WAN-scale scalability

### Remote Data: Remote File System
Remote File Access (Eg. `NFS`, `CIFS`)
	primary or proabably one secondary server
	+ simplicity
Distributed File System: data spreaded across numerous servers (Eg. `Ceph`)
	+ performance, scalability
	- much larger complexity

### Remote Data: Security
Privacy and integrity for data on the network: encrypt all data sent over network

Authentication of remote users
1. **Anonymous access**: all files are available to all users, may be limited to read-only access
   + simple implementation
   - can't provide information privacy

2. **Peer-to-peer security**: all participating nodes are known to be trusted peers
   every client reports self-identity when accessing server, which just believes it to be true
   Example: basic `NFS`
   + simple implementation
   - doesn't work in heterogeneous OS environment
   - Has to keep a Universal user registry, which is not scalable
   
3. **Server authenticated approach**: client agent authenticates to each server, based on credentials produced by server
   +/- similar to (2)
   - no automatic fail-over if server dies
   
4. **Domain authentication approach**: independent authentication of client & server
   each authenticates with independent authentication service: authentication service returns credentials (goes into Access Control List) or capabilities
   + privacy & integrity

### Remote Data: Reliability and Availability
Typically reduce probbability of data loss by some form of redundancy, and ability to automatically recover after failure
	Eg. Data mirroring: back-side (server-side) or front-side (client-side)
	
Approaches to achieve reliability:
	*Data mirroring*: multiple copies; requires much space
	*Parity*: requires full strip write buffering
	*Erasure coding*: generate longer data stream than the original, and being able to recover data from a subset of these bytes; expensive reads & writes
	
Fail-over: Filure Detection & Rebind:
	client-driven recovery: client detects server failure and reconnects to successor server
	transparent failure recovery: system detects server failure and sucessor server assumes primary's IP address, for example.
	*Stateful protocols*: each operation depends on previous operations
		Eg. `TCP`
		Replacement server must obtain session state
	*Stateless protocols*: Eg. `HTTP`

### Remote Data: Performance
Caching for reads
	client-side caching: cache data permanently stored at the server at the client; reduces network traffic
	server-side caching: reduces disk delays
	*whole file caching*: `AFS`; *block caching*: `NFS`

Caching for writes
	*write-back cache*: create fewer, larger network and disk wrties. 
	*whole file updates*: no writes sent to server until `close()` or `fsync()`. Enable atomic updates, but may lead to potential problems of inconsistency
	
Cost of consistency: multi-writer distributed caching is hard

Performance measurement for Distributed Systems:
*Recover Time*: Mean Time to Failure, Mean Time to Repair
