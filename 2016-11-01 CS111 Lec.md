## COM SCI 111, Lec 11/01/2016
### Increasing Efficiency for Synchronization
- Reducing time in critical section
  Doing memory allocation before taking the lock
  Doing I/O after releasing the lock
  Eg. Inserting a new node to a linked list
  
- Reducing frequency of entering critical sections
  Possibly by batch operations
  Eg. *Sloopy counters*: move most updates to a private resource
  Alternative approach: do not keep a shared counter, whoever needs that data goes into each thread's private counter and sum it up
  
- Remove requirement for full exclusivity
  Eg. Read/write locks: only writers require exclusive access but reads are usually much more than writes.
  
- Spread requests over more resources
  *Coarse grained*: one lock for many objects
  *Fine grained*: one lock per object or sub-pool
  A few operations may still need to lock multiple objects/pools
  
Handling priority inversion problem - *Priority inheritance*: a general solution
	Temporarily increase the priority of the low-priority task (Eg. the meteorological task). When lock is released, drop its priority back to normal.  
	
### Deadlock
A situation where two entities have each locked some resource, and each needs the other's locked resource to continue. Neither will unlock till they lock both resources; hence neither can ever make progress.

*The Dining Philosopher Problem*: when everyone pick up one fork at the same time, but they have to wait the other fork forever.

Finding deadlocks through debugging is very difficult (indeterministic result); better to prevent at design time.
Deadlocks may not be obvious: process resource needs are ever-changing; modern software depends on many services; services encapsulate much complexity

Deadlocks and type of resources:
**Commodity Resources**: clients need an amount of it. (Eg. Memory)
	Deadlocks result from over-commitment; can be avoided in resource manager
**General Resources**: clients need a specific instance of something (Eg. a file)
	Deadlocks result from specific dependency relationships; better be avoided at design time
	
Four basic conditions for deadlocks:
**Mutual exclusion**: the resource can only be used by one entity at a time
**Incremental allocation**: processes/threads are allowed to ask for resources whenever they want, as opposed to getting everything they need before they start.
**No pre-emption**: [Not scheduler pre-emption] When an entity has reserved a resource, you cannot take it away from him.
**Circular waiting**: A waits on the exclusive resource hold by B; B waits on that hold by C, etc. C waits on the resource hold by A.

Deadlock from commodity resource Example: system swap manager
	Invoked to swap out processes to free up memory, but may need to allocate memory to build I/O requests.
	Solution: pre-allocate and reserve a few request buffers in memory, not usuable by any other processes
	
### Deadlock: Avoidance
For commodity resources: advance reservation
   Resource manager only grants reservations if resources are available. Over-subscriptions are detected early
   *Overbooking* VS. *Under Utilization*: two ways for OS to grant resources to processes, which usually request for more than they actually neeed.
   Apps must deal with reservation failures gracefully: Eg. refuse to perform new request but continue running, return error messages, etc.
   
For general resources:
1. Mutual exclusion
   Use shareable resource, probably maintained with atomic insturctions
   Use private resources for each thread/process
   
2. Incremental allocation
   Request for all resources in a single operation
   Issue non-blocking requests: a request that cannot be satisfied will immediately fail
   Release all held locks prior to being blocked
   
3. No Pre-emption
   "Lease" resources with time-outs rather than giving it to a process for infinite time; revocation of resources is enforced
   However, some resources may be damaged by *lock breaking*, when the previous owner was executing in the middle of critical section
   
4.Circular dependencies
	*total resource ordering*: all reqeusters allocate resources in same order
	*Lock dances*: we may first release Lock 2, hold Lock 1 and then re-hold Lock 2, to observe the ordering of resource locks.
	
One more deadlock solution: ignore the problem
Implementing deadlock detection: identify all resources that can be locked, and maintain wait-for graphs

### Dealing with General Synchronization Bugs
*Service/application health monitoring*: monitor application progress; if response takes too long, declare service "hung".

It deals with bad situations which are not really deadlock:
- *Live-lock*: one process is running normally but it can never free its locked resource, as the signal it's waiting will never come.
- Sleep-wake problem: a sleeping process will never be waked up
- Priority inversion hangs

Failure recovery methodologies:
*Partial/Warm/Cold restarts*
